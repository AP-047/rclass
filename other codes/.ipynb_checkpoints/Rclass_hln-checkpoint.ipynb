{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5418a007-3fc4-4f0a-a27c-e40364035ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "\n",
    "class RationalClassifier:\n",
    "    def __init__(self, numerator_degree, denominator_degree, n_components, delta=1e-5, precision=1e-6):\n",
    "        self.numerator_degree = numerator_degree\n",
    "        self.denominator_degree = denominator_degree\n",
    "        self.n_components = n_components\n",
    "        self.delta = delta\n",
    "        self.precision = precision\n",
    "        self.alpha = None\n",
    "        self.beta = None\n",
    "        self.z = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the rational classifier by solving for optimal alpha, beta, and z.\n",
    "        \"\"\"\n",
    "        # Generate rational function matrices\n",
    "        G_matrix, H_matrix = generate_rational_function_matrix(\n",
    "            self.numerator_degree, self.denominator_degree, self.n_components, X\n",
    "        )\n",
    "        \n",
    "        # Use bisection method to find optimal z\n",
    "        self.z = bisection_method(\n",
    "            uL=0, uH=100, G_matrix=G_matrix, H_matrix=H_matrix, y=y, precision=self.precision\n",
    "        )\n",
    "        \n",
    "        # Solve for alpha and beta at the optimal z\n",
    "        result = solve_lp(self.z, G_matrix, H_matrix, y, delta=self.delta)\n",
    "        if not result.success:\n",
    "            raise ValueError(\"Linear programming failed to converge.\")\n",
    "         \n",
    "        \n",
    "        # Extract alpha and beta coefficients\n",
    "        self.alpha = result.x[1 : 1 + G_matrix.shape[1]]\n",
    "        self.beta = result.x[1 + G_matrix.shape[1] :]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for input data X.\n",
    "        \"\"\"\n",
    "        # Generate rational function matrices for prediction\n",
    "        G_matrix, H_matrix = generate_rational_function_matrix(\n",
    "            self.numerator_degree, self.denominator_degree, self.n_components, X\n",
    "        )\n",
    "        \n",
    "        # Compute rational function values\n",
    "        numerator = np.dot(G_matrix, self.alpha)\n",
    "        denominator =  1e-5+ np.dot(H_matrix, self.beta)\n",
    "        rational_values = numerator / denominator\n",
    "        \n",
    "        # Apply threshold to classify\n",
    "        predictions = (rational_values > 0.3).astype(int)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Generating multi-indices\n",
    "def r_multi_indices(n, d):\n",
    "    if n == 1:\n",
    "        yield (d,)\n",
    "    else:\n",
    "        for k in range(d + 1):\n",
    "            for c in r_multi_indices(n - 1, k):\n",
    "                yield (d - k, *c)\n",
    "\n",
    "\n",
    "def generate_multi_indices(n, d):\n",
    "    from itertools import chain\n",
    "    return list(chain(*[list(r_multi_indices(n, _)) for _ in range(d + 1)]))\n",
    "\n",
    "\n",
    "# Generating rational function matrices\n",
    "def generate_rational_function_matrix(numerator_degree, denominator_degree, n_components, dataset):\n",
    "    G_indices = generate_multi_indices(n_components, numerator_degree)\n",
    "    H_indices = generate_multi_indices(n_components, denominator_degree)\n",
    "\n",
    "    G_matrix = np.zeros((len(dataset), len(G_indices)))\n",
    "    H_matrix = np.zeros((len(dataset), len(H_indices)))\n",
    "\n",
    "    for i, data_point in enumerate(dataset):\n",
    "        G_matrix[i] = [np.prod([data_point[k] ** idx[k] for k in range(len(data_point))]) for idx in G_indices]\n",
    "        H_matrix[i] = [np.prod([data_point[k] ** idx[k] for k in range(len(data_point))]) for idx in H_indices]\n",
    "\n",
    "    return G_matrix, H_matrix\n",
    "\n",
    "\n",
    "# Linear programming setup\n",
    "def solve_lp(z, G_matrix, H_matrix, y, delta=1e-5):\n",
    "    length_dataset = len(y)\n",
    "    num_alpha = G_matrix.shape[1]\n",
    "    num_beta = H_matrix.shape[1]\n",
    "\n",
    "    c = [1] + [0] * (num_alpha + num_beta)\n",
    "    \n",
    "    A_ub, b_ub = [], []\n",
    "\n",
    "    for i in range(length_dataset):\n",
    "        G_row = G_matrix[i]\n",
    "        H_row = H_matrix[i]\n",
    "        y_i = y[i]\n",
    "\n",
    "        # First constraint\n",
    "        A_ub.append([-1] + [-g for g in G_row] + [(y_i - z) * h for h in H_row])\n",
    "        b_ub.append(0)\n",
    "\n",
    "        # Second constraint\n",
    "        A_ub.append([-1] + [g for g in G_row] + [-(y_i - z) * h for h in H_row])\n",
    "        b_ub.append(0)\n",
    "\n",
    "        # Positivity constraint\n",
    "        A_ub.append([0] + [0] * num_alpha + [-h for h in H_row])\n",
    "        b_ub.append(-delta)\n",
    "\n",
    "    bounds = [(0, None)] + [(None, None)] * (num_alpha + num_beta)\n",
    "    result = linprog(c=c, A_ub=np.array(A_ub), b_ub=np.array(b_ub), bounds=bounds, method=\"highs\")\n",
    "   # print(f\"Optimization result: {result}\")\n",
    "    return result\n",
    "\n",
    "# Bisection method to find optimal z\n",
    "#def bisection_method(uL, uH, G_matrix, H_matrix, y, precision=1e-6):\n",
    "   # while (uH - uL) > precision:\n",
    "     #   z = (uH + uL) / 2\n",
    "      #  result = solve_lp(z, G_matrix, H_matrix, y)\n",
    "      #  if result.success:\n",
    "        #    uH = z\n",
    "       # else:\n",
    "         #   uL = z\n",
    "  #  return uH\n",
    "def bisection_method(uL, uH, G_matrix, H_matrix, y, precision=1e-6):\n",
    "  \n",
    "    while (uH - uL) > precision:\n",
    "        z = (uH + uL) / 2\n",
    "        result = solve_lp(z, G_matrix, H_matrix, y)\n",
    "      \n",
    "        if result.success:\n",
    "            uH = z\n",
    "        else:\n",
    "            uL = z\n",
    "\n",
    "    return uH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72782e8-581f-4828-9d01-236945b61d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset (first 20 rows):\n",
      "    feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
      "0    0.374540   0.950714   0.731994   0.598658   0.156019   0.155995   \n",
      "1    0.020584   0.969910   0.832443   0.212339   0.181825   0.183405   \n",
      "2    0.611853   0.139494   0.292145   0.366362   0.456070   0.785176   \n",
      "3    0.607545   0.170524   0.065052   0.948886   0.965632   0.808397   \n",
      "4    0.122038   0.495177   0.034389   0.909320   0.258780   0.662522   \n",
      "5    0.969585   0.775133   0.939499   0.894827   0.597900   0.921874   \n",
      "6    0.388677   0.271349   0.828738   0.356753   0.280935   0.542696   \n",
      "7    0.772245   0.198716   0.005522   0.815461   0.706857   0.729007   \n",
      "8    0.863103   0.623298   0.330898   0.063558   0.310982   0.325183   \n",
      "9    0.119594   0.713245   0.760785   0.561277   0.770967   0.493796   \n",
      "10   0.031429   0.636410   0.314356   0.508571   0.907566   0.249292   \n",
      "11   0.289751   0.161221   0.929698   0.808120   0.633404   0.871461   \n",
      "12   0.807440   0.896091   0.318003   0.110052   0.227935   0.427108   \n",
      "13   0.417411   0.222108   0.119865   0.337615   0.942910   0.323203   \n",
      "14   0.962447   0.251782   0.497249   0.300878   0.284840   0.036887   \n",
      "15   0.908266   0.239562   0.144895   0.489453   0.985650   0.242055   \n",
      "16   0.367783   0.632306   0.633530   0.535775   0.090290   0.835302   \n",
      "17   0.677564   0.016588   0.512093   0.226496   0.645173   0.174366   \n",
      "18   0.341066   0.113474   0.924694   0.877339   0.257942   0.659984   \n",
      "19   0.093103   0.897216   0.900418   0.633101   0.339030   0.349210   \n",
      "\n",
      "    feature_7  feature_8  feature_9  feature_10  label  \n",
      "0    0.058084   0.866176   0.601115    0.708073      0  \n",
      "1    0.304242   0.524756   0.431945    0.291229      1  \n",
      "2    0.199674   0.514234   0.592415    0.046450      1  \n",
      "3    0.304614   0.097672   0.684233    0.440152      1  \n",
      "4    0.311711   0.520068   0.546710    0.184854      1  \n",
      "5    0.088493   0.195983   0.045227    0.325330      0  \n",
      "6    0.140924   0.802197   0.074551    0.986887      1  \n",
      "7    0.771270   0.074045   0.358466    0.115869      0  \n",
      "8    0.729606   0.637557   0.887213    0.472215      0  \n",
      "9    0.522733   0.427541   0.025419    0.107891      0  \n",
      "10   0.410383   0.755551   0.228798    0.076980      0  \n",
      "11   0.803672   0.186570   0.892559    0.539342      0  \n",
      "12   0.818015   0.860731   0.006952    0.510747      1  \n",
      "13   0.518791   0.703019   0.363630    0.971782      1  \n",
      "14   0.609564   0.502679   0.051479    0.278646      1  \n",
      "15   0.672136   0.761620   0.237638    0.728216      1  \n",
      "16   0.320780   0.186519   0.040775    0.590893      0  \n",
      "17   0.690938   0.386735   0.936730    0.137521      1  \n",
      "18   0.817222   0.555201   0.529651    0.241852      0  \n",
      "19   0.725956   0.897110   0.887086    0.779876      1  \n",
      "\n",
      "Training Set:\n",
      "X_train shape: (80, 10), y_train shape: (80,)\n",
      "\n",
      "Testing Set:\n",
      "X_test shape: (20, 10), y_test shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 100 samples with 10 features each, as pixel intensities (random between 0 and 1)\n",
    "X = np.random.rand(100, 10)\n",
    "\n",
    "# Generate binary labels (0 or 1)\n",
    "y = np.random.randint(0, 2, size=100)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Show the first 20 rows of the dataset\n",
    "df = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(10)])\n",
    "df['label'] = y\n",
    "\n",
    "print(\"Dataset (first 20 rows):\")\n",
    "print(df.head(20))\n",
    "\n",
    "# Show the train and test splits (you can adjust the print below to check the training set or test set)\n",
    "print(\"\\nTraining Set:\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "print(\"\\nTesting Set:\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b07ab52-2818-4783-822f-e8c774977a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha coefficients: [ 1.55180743e-04  0.00000000e+00  3.37086189e-04  0.00000000e+00\n",
      " -6.88056056e-04  0.00000000e+00  8.07643011e-04  5.34456231e-05\n",
      " -1.71219184e-03  3.90154473e-04 -3.75665249e-04  9.46964107e-04\n",
      "  1.14622456e-03 -1.23012361e-03 -1.13749179e-03  8.15511208e-04\n",
      " -7.66542723e-04 -7.22546658e-04 -1.69187529e-04  5.79402149e-04\n",
      " -6.41701179e-04  9.24524284e-04 -1.26766699e-03 -5.08333707e-04\n",
      " -1.23923979e-03  6.74006434e-04  1.01206994e-05 -1.10004754e-03\n",
      " -5.19508805e-04 -4.46380257e-04  7.06530363e-04  3.84173778e-04\n",
      "  6.25532406e-04 -6.27317057e-04 -1.02350505e-03  1.92688897e-03\n",
      "  3.24057291e-04  3.50597391e-04  1.25523258e-03 -2.65860262e-04\n",
      " -4.09054248e-04 -2.07176380e-04  1.00776107e-03  5.17843919e-06\n",
      " -5.40465632e-06  1.28966609e-04 -5.49211949e-04  0.00000000e+00\n",
      "  3.13741880e-04  5.50813898e-04 -1.98209721e-04  2.32032625e-04\n",
      " -9.11754214e-04  8.09213775e-04 -1.28582878e-05 -3.59974852e-04\n",
      "  8.02864708e-04  8.26354531e-04 -6.30575291e-04  7.14164957e-04\n",
      " -1.82886798e-04 -2.77688965e-04  5.81923909e-04 -9.37040481e-04\n",
      "  6.22360901e-04  1.83100878e-04]\n",
      "Beta coefficients: [ 4.40616555e-04  0.00000000e+00  1.17709617e-03 -7.19388605e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.10661174e-04\n",
      " -7.46071682e-04 -8.17956344e-04  0.00000000e+00 -4.05706079e-04\n",
      "  8.10692977e-05 -3.35161394e-04  0.00000000e+00  1.09842137e-03\n",
      " -1.15120529e-03  2.93278702e-04 -8.15368397e-05  6.05490310e-04\n",
      "  0.00000000e+00  0.00000000e+00 -9.61155972e-04  0.00000000e+00\n",
      " -7.03958873e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -5.04324755e-04  0.00000000e+00 -5.84536610e-06  1.10220331e-03\n",
      "  0.00000000e+00 -1.72073556e-04 -3.33115638e-05  0.00000000e+00\n",
      "  1.12666969e-03  8.29200132e-04  1.37982544e-04  0.00000000e+00\n",
      " -3.54571381e-04  0.00000000e+00 -5.72547997e-04  0.00000000e+00\n",
      " -6.23613784e-04  8.36419147e-05  0.00000000e+00 -1.09256906e-03\n",
      "  5.24114010e-04  0.00000000e+00 -3.10066948e-04  1.30247883e-03\n",
      "  3.08430212e-04  0.00000000e+00 -4.20844749e-04 -6.86029936e-04\n",
      "  0.00000000e+00  4.43599629e-04  0.00000000e+00  0.00000000e+00\n",
      "  7.31771074e-04 -8.90289951e-04  9.09686970e-04  6.92156542e-04\n",
      "  0.00000000e+00 -1.50749634e-04]\n",
      "Optimal z value: 7.450580596923828e-07\n",
      "accuracy: 0.65\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.7272727272727273\n",
      "F1-score: 0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix,recall_score\n",
    "\n",
    "# Initialize RationalClassifier with chosen parameters\n",
    "numerator_degree = 2\n",
    "denominator_degree = 2\n",
    "n_components = 10  # Number of components (can be adjusted)\n",
    "\n",
    "classifier = RationalClassifier(numerator_degree=numerator_degree,\n",
    "                               denominator_degree=denominator_degree,\n",
    "                               n_components=n_components)\n",
    "\n",
    "# Train the classifier on the dataset\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions from the RationalClassifier\n",
    "rational_predictions = classifier.predict(X_test)\n",
    "\n",
    "print(\"Alpha coefficients:\", classifier.alpha)\n",
    "print(\"Beta coefficients:\", classifier.beta)\n",
    "print(\"Optimal z value:\", classifier.z)\n",
    "\n",
    "\n",
    "# Calculate precision, recall, and F1 score for both classes (0 and 1)\n",
    "accuracy = accuracy_score(y_test, rational_predictions)\n",
    "precision = precision_score(y_test, rational_predictions)\n",
    "recall = recall_score(y_test, rational_predictions)\n",
    "f1 = f1_score(y_test, rational_predictions)\n",
    "\n",
    "# Print precision, recall, and F1 score\n",
    "print(f\"accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e410434-eedb-474d-ad37-971b5ff93cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5\n",
      "Precision: 0.6\n",
      "Recall: 0.2727272727272727\n",
      "F1-score: 0.375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix,recall_score\n",
    "\n",
    "\n",
    "# Train a Logistic Regression model with class weights\n",
    "model = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate precision, recall, and F1 score for both classes (0 and 1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "# Print precision, recall, and F1 score\n",
    "print(f\"accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bc3fd8f-6648-48be-b28a-9735a3300f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Comparison (first 20):\n",
      "\n",
      "    Logistic Regression Prediction  RationalClassifier Prediction  Actual\n",
      "0                                1                              0       1\n",
      "1                                0                              1       1\n",
      "2                                0                              0       0\n",
      "3                                0                              1       1\n",
      "4                                0                              0       1\n",
      "5                                0                              1       0\n",
      "6                                1                              1       1\n",
      "7                                0                              1       0\n",
      "8                                1                              0       0\n",
      "9                                0                              0       0\n",
      "10                               0                              1       0\n",
      "11                               0                              0       0\n",
      "12                               0                              1       1\n",
      "13                               0                              0       1\n",
      "14                               0                              1       1\n",
      "15                               0                              1       1\n",
      "16                               0                              1       1\n",
      "17                               1                              1       0\n",
      "18                               1                              1       1\n",
      "19                               0                              0       0\n"
     ]
    }
   ],
   "source": [
    "# Optionally, compare the RationalClassifier predictions with the Logistic Regression predictions\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Logistic Regression Prediction': y_pred[:20],\n",
    "    'RationalClassifier Prediction': rational_predictions[:20],\n",
    "    'Actual': y_test[:20]\n",
    "})\n",
    "\n",
    "print(\"\\nPrediction Comparison (first 20):\\n\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485dc2d-5710-4d41-a96d-80dccb4beb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56eb55d-adc5-4f32-a665-66018108fae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
