{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0f260a-39de-40b3-88e4-b86ec9388c0b",
   "metadata": {},
   "source": [
    "🍁 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9d1051-7755-406d-add7-6d75c6b80251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28), y_train shape: (60000,)\n",
      "x_train_flat shape: (60000, 784), y_train shape: (60000,)\n",
      "x_train_subset shape: (500, 784)\n",
      "x_train_pca shape: (500, 10)\n",
      "variance = 0.5235613405019189\n",
      "x_train_binary shape: (500, 10)\n",
      "[[1 1 0 ... 1 1 1]\n",
      " [1 1 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 1 1 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 1 1 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Import\n",
    "(x_train, y_train), (_, _) = mnist.load_data()\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "# 1. Flatten\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
    "print(f\"x_train_flat shape: {x_train_flat.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "# 2. Subsets\n",
    "subset_size = 500\n",
    "x_train_subset = x_train_flat[:subset_size]\n",
    "y_train_subset = y_train[:subset_size]\n",
    "print(f\"x_train_subset shape: {x_train_subset.shape}\")\n",
    "\n",
    "# 3. PCA\n",
    "n_components = 10\n",
    "pca = PCA(n_components=n_components)\n",
    "x_train_pca = pca.fit_transform(x_train_subset)\n",
    "print(f\"x_train_pca shape: {x_train_pca.shape}\")\n",
    "variance = np.sum(pca.explained_variance_ratio_)\n",
    "print(f\"variance = {variance}\")  # Verify how much variance is retained\n",
    "\n",
    "# # 4. Normalize\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# x_train_norm = scaler.fit_transform(x_train_pca)\n",
    "# print(f\"x_train_norm shape: {x_train_norm.shape}\")\n",
    "# x_train_norm.shape[0]\n",
    "\n",
    "# 4. Binarize\n",
    "threshold_value = 0\n",
    "x_train_norm = (x_train_pca > threshold_value).astype(int)\n",
    "print(f\"x_train_binary shape: {x_train_norm.shape}\")\n",
    "print(x_train_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5bce02-b9ac-4a1a-8503-8b77fc5478c4",
   "metadata": {},
   "source": [
    "ssssss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "579b1665-4bce-4d1b-b2bd-c72553cd45d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisection_loop(x_train_norm, y_binary, uL, uH, precision, n_components):\n",
    "    optimal_alpha, optimal_beta, optimal_theta = None, None, None\n",
    "    z_values = []\n",
    "\n",
    "    while uH - uL > precision:\n",
    "        z = (uL + uH) / 2\n",
    "        z_values.append(z)\n",
    "\n",
    "        # Feasibility Check\n",
    "        feasible, alpha_coefficients, beta_coefficients, theta = check_feasibility_and_compute_coefficients(\n",
    "            z, x_train_norm, y_binary, n_components\n",
    "        )\n",
    "\n",
    "        # Update bounds based on feasibility\n",
    "        if feasible:\n",
    "            uH = z\n",
    "            optimal_alpha, optimal_beta, optimal_theta = alpha_coefficients, beta_coefficients, theta\n",
    "        else:\n",
    "            uL = z\n",
    "\n",
    "    return uH, optimal_alpha, optimal_beta, optimal_theta, z_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8442ea3-2526-4f71-95b3-abd038ced3fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_binary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# n_components = 2\u001b[39;00m\n\u001b[1;32m      5\u001b[0m uH, optimal_alpha, optimal_beta, optimal_theta, z_values \u001b[38;5;241m=\u001b[39m bisection_loop(\n\u001b[0;32m----> 6\u001b[0m     x_train_norm, \u001b[43my_binary\u001b[49m, uL, uH, precision, n_components\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal z: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Optimal Coefficients: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_alpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_beta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_binary' is not defined"
     ]
    }
   ],
   "source": [
    "uL = 0\n",
    "uH = 10\n",
    "precision = 1e-3\n",
    "# n_components = 2\n",
    "uH, optimal_alpha, optimal_beta, optimal_theta, z_values = bisection_loop(\n",
    "    x_train_norm, y_binary, uL, uH, precision, n_components\n",
    ")\n",
    "print(f\"Final z: {uH}, Optimal Coefficients: {optimal_alpha}, {optimal_beta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2fc00d-c3be-421e-bbd3-e61cfed14022",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PCA' object has no attribute 'components_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m n_components \u001b[38;5;241m=\u001b[39m Integer(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     25\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39mn_components)\n\u001b[0;32m---> 26\u001b[0m x_test_pca \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test_subset\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use the same PCA model from training\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Binarize\u001b[39;00m\n\u001b[1;32m     29\u001b[0m threshold_value \u001b[38;5;241m=\u001b[39m Integer(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sagemath-env/lib/python3.11/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/sagemath-env/lib/python3.11/site-packages/sklearn/decomposition/_base.py:134\u001b[0m, in \u001b[0;36m_BasePCA.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply dimensionality reduction to X.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    X is projected on the first principal components previously extracted\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        is the number of samples and `n_components` is the number of the components.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponents_\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplained_variance_)\n\u001b[1;32m    136\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    138\u001b[0m     X \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m         X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m         reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    144\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PCA' object has no attribute 'components_'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.datasets import mnist\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Directory containing saved models\n",
    "models_dir = \"/home/ajay2425/rclass/models_grb1/\"\n",
    "\n",
    "# Load Test Data\n",
    "(_, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess Test Data\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)  # Flatten\n",
    "x_test_subset = x_test_flat[:1000]  # Test on a smaller subset\n",
    "y_test_subset = y_test[:1000]\n",
    "\n",
    "# Apply PCA (same as training)\n",
    "n_components = 10\n",
    "pca = PCA(n_components=n_components)\n",
    "x_test_pca = pca.transform(x_test_subset)  # Use the same PCA model from training\n",
    "\n",
    "# Binarize\n",
    "threshold_value = 0\n",
    "x_train_norm = (x_train_pca > threshold_value).astype(int)\n",
    "print(f\"x_train_norm shape: {x_train_norm.shape}\")\n",
    "\n",
    "# # Normalize\n",
    "# x_test_norm = (x_test_pca > 0).astype(int)\n",
    "\n",
    "# Accuracy, Time, and Computations\n",
    "overall_start_time = time.time()  # Track total time\n",
    "accuracies = []\n",
    "computation_counts = []\n",
    "\n",
    "# Test each digit classifier\n",
    "for digit in range(10):\n",
    "    print(f\"Testing classifier for digit {digit}...\")\n",
    "\n",
    "    # Load the model\n",
    "    model_path = f\"{models_dir}/classifier_{digit}.pkl\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model for digit {digit} not found! Skipping...\")\n",
    "        continue\n",
    "\n",
    "    with open(model_path, \"rb\") as file:\n",
    "        model_data = pickle.load(file)\n",
    "\n",
    "    alpha = model_data[\"alpha\"]\n",
    "    beta = model_data[\"beta\"]\n",
    "    theta = model_data[\"theta\"]\n",
    "    multi_indices = model_data[\"multi_indices\"]\n",
    "\n",
    "    # Start timer for this classifier\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Test predictions using the rational function\n",
    "    num_coefficients = len(alpha)\n",
    "    n = x_test_norm.shape[1]  # Number of features\n",
    "    d = model_data[\"degree\"]  # Degree of the rational function\n",
    "\n",
    "    # Compute G and H matrices for the test data\n",
    "    G, H, _ = construct_G_H_matrices(x_test_norm, n, d)\n",
    "\n",
    "    # Predict for each test sample\n",
    "    y_pred_binary = []\n",
    "    for i in range(len(x_test_norm)):\n",
    "        G_x = np.dot(G[i], alpha)  # αᵀG(xᵢ)\n",
    "        H_x = np.dot(H[i], beta)   # βᵀH(xᵢ)\n",
    "        rational_value = G_x / H_x if H_x != 0 else float(\"inf\")\n",
    "        y_pred_binary.append(1 if rational_value < theta else 0)\n",
    "\n",
    "    # End timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Convert binary predictions back to digit predictions\n",
    "    y_pred = [digit if pred == 1 else -1 for pred in y_pred_binary]\n",
    "\n",
    "    # Filter only relevant samples\n",
    "    relevant_indices = np.where((y_test_subset == digit) | (np.array(y_pred) == digit))[0]\n",
    "    y_true_filtered = y_test_subset[relevant_indices]\n",
    "    y_pred_filtered = np.array(y_pred)[relevant_indices]\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Estimate computations (e.g., matrix multiplications)\n",
    "    computations = len(x_test_norm) * num_coefficients * 2  # G_x and H_x calculations\n",
    "    computation_counts.append(computations)\n",
    "\n",
    "    print(f\"Digit {digit}: Accuracy = {accuracy * 100:.2f}%, Time = {end_time - start_time:.2f}s, Computations = {computations}\")\n",
    "\n",
    "# Calculate total testing time\n",
    "overall_end_time = time.time()\n",
    "total_time = overall_end_time - overall_start_time\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nOverall Testing Time: {total_time:.2f}s\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracies) * 100:.2f}%\")\n",
    "print(f\"Total Computations: {np.sum(computation_counts):,}\")\n",
    "\n",
    "# Save results for comparison\n",
    "results = {\n",
    "    \"accuracies\": accuracies,\n",
    "    \"computation_counts\": computation_counts,\n",
    "    \"total_time\": total_time\n",
    "}\n",
    "# with open(f\"{models_dir}/test_results.json\", \"w\") as file:\n",
    "#     json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae674413-5095-437f-a683-356f72d80f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.4",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
