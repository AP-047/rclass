{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad4e77f-a086-42c9-93aa-090d26a5032c",
   "metadata": {},
   "source": [
    "üåøPreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e6b5cf5-f600-4a1c-bda8-2451479d6075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28), y_train shape: (60000,)\n",
      "1. x_train_flat shape: (60000, 784), y_train shape: (60000,)\n",
      "2. x_train_subset shape: (10000, 784)\n",
      "3. x_train_pca shape: (10000, 15)\n",
      "variance = 0.5772104890520655\n",
      "(train pca model saved)\n",
      "4. x_train_norm shape: (10000, 15)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "# load local mnist data file\n",
    "mnist_path = \"/home/ajay2425/rclass/mnist_dataset/mnist.npz\"\n",
    "with np.load(mnist_path) as data:\n",
    "    x_train = data[\"x_train\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "# 1. Flatten\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
    "print(f\"1. x_train_flat shape: {x_train_flat.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "# 2. Subsets (with equal distribution)\n",
    "subset_size_per_digit = 1000\n",
    "unique_digits = np.unique(y_train)\n",
    "\n",
    "x_train_balanced = []\n",
    "y_train_balanced = []\n",
    "\n",
    "for digit in unique_digits:\n",
    "    # Get indices of all samples of the current digit\n",
    "    digit_indices = np.where(y_train == digit)[0]\n",
    "    # Randomly sample subset_size_per_digit indices\n",
    "    sampled_indices = np.random.choice(digit_indices, subset_size_per_digit, replace=False)\n",
    "    # Append the sampled data\n",
    "    x_train_balanced.append(x_train_flat[sampled_indices])\n",
    "    y_train_balanced.append(y_train[sampled_indices])\n",
    "\n",
    "# Combine and shuffle the balanced dataset\n",
    "x_train_balanced = np.vstack(x_train_balanced)\n",
    "y_train_balanced = np.hstack(y_train_balanced)\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffled_indices = np.random.permutation(len(y_train_balanced))\n",
    "x_train_subset = x_train_balanced[shuffled_indices]\n",
    "y_train_subset = y_train_balanced[shuffled_indices]\n",
    "print(f\"2. x_train_subset shape: {x_train_subset.shape}\")\n",
    "\n",
    "# 3. PCA\n",
    "n_components = 15\n",
    "d = 2 # degree\n",
    "pca = PCA(n_components=n_components)\n",
    "x_train_pca = pca.fit_transform(x_train_subset)\n",
    "print(f\"3. x_train_pca shape: {x_train_pca.shape}\")\n",
    "variance = np.sum(pca.explained_variance_ratio_)\n",
    "print(f\"variance = {variance}\")\n",
    "\n",
    "# Save the trained PCA model\n",
    "pca_model_path = \"/home/ajay2425/rclass/models/models_grb/trained_pca.pkl\" # main\n",
    "# pca_model_path = \"/home/ajay2425/rclass/models/add/trained_pca.pkl\" # temp.\n",
    "with open(pca_model_path, \"wb\") as file:\n",
    "    pickle.dump(pca, file)\n",
    "print(f\"(train pca model saved)\")\n",
    "\n",
    "\n",
    "# 4. Normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_train_norm = scaler.fit_transform(x_train_pca)\n",
    "print(f\"4. x_train_norm shape: {x_train_norm.shape}\")\n",
    "\n",
    "# # 4. Binarize\n",
    "# threshold_value = 0\n",
    "# x_train_norm = (x_train_pca > threshold_value).astype(int)\n",
    "# print(f\"4. x_train_norm shape: {x_train_norm.shape}\")\n",
    "# # print(x_train_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b28b24-5624-4723-96e8-8e264b30629c",
   "metadata": {},
   "source": [
    "üåøCheck Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82ebaf3-2131-4139-acfb-090eb7a74fb1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory: /home/ajay2425/rclass/models/models_grb/\n",
      "Directory already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the directory\n",
    "models_dir = \"/home/ajay2425/rclass/models/models_grb/\" # main\n",
    "# pca_model_path = \"/home/ajay2425/rclass/models/add/trained_pca.pkl\" # temp.\n",
    "print(f\"Using existing directory: {models_dir}\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(models_dir):\n",
    "    print(\"Directory already exists.\")\n",
    "else:\n",
    "    print(\"Directory does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e26809-7a30-41b2-b04e-c174f9caf870",
   "metadata": {},
   "source": [
    "üåøGenerate multi-Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6f3d3b-a38c-4aeb-9ff3-fcdb709b203a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_of_coeff = 136\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "def r_multi_indices(n, d):\n",
    "    if n == 1:\n",
    "        yield (d,)\n",
    "    else:\n",
    "        for k in range(d + 1):\n",
    "            for c in r_multi_indices(n - 1, k):\n",
    "                yield (d - k, *c)\n",
    "\n",
    "def generate_multi_indices(n, d):\n",
    "    from itertools import chain\n",
    "    return list(chain(*[list(r_multi_indices(n, _)) for _ in range(d + 1)]))\n",
    "\n",
    "c = generate_multi_indices(n_components, d)\n",
    "print(f\"no_of_coeff =\", len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bff7d1-259a-4542-9616-54c44cdbb37b",
   "metadata": {},
   "source": [
    "üåøGenerate Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599aa024-5b41-4081-ae03-f9e24537c783",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def construct_G_H_matrices(x_train_norm, n, d):\n",
    "    num_data_points = x_train_norm.shape[0]\n",
    "    multi_indices = generate_multi_indices(n, d)\n",
    "    num_coefficients = len(multi_indices)\n",
    "\n",
    "    # Initialize G and H matrices\n",
    "    G = []\n",
    "    H = []\n",
    "\n",
    "    # Construct G and H using multi-indices\n",
    "    for i in range(num_data_points):\n",
    "        G_row = []\n",
    "        H_row = []\n",
    "        for idx in multi_indices:\n",
    "            term = np.prod([x_train_norm[i, k] ** idx[k] for k in range(n)])\n",
    "            G_row.append(term)\n",
    "            H_row.append(term)\n",
    "        G.append(G_row)\n",
    "        H.append(H_row)\n",
    "\n",
    "    # Convert G and H to NumPy arrays\n",
    "    G = np.array(G)\n",
    "    H = np.array(H)\n",
    "\n",
    "    # # Normalize G and H row-wise for numerical stability\n",
    "    # G = G / np.linalg.norm(G, axis=1, keepdims=True)\n",
    "    # H = H / np.linalg.norm(H, axis=1, keepdims=True)\n",
    "\n",
    "    return G, H, multi_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adc6f1-2cf6-40a2-ac7a-7e84932e7451",
   "metadata": {},
   "source": [
    "üåøProblem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32e4983-421b-434b-a3e1-6a6f88717fc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from gurobipy import Model, GRB, Env\n",
    "\n",
    "# Suppress Gurobi logs globally\n",
    "env = Env(empty=True)\n",
    "env.setParam(\"OutputFlag\", 0)  # Suppress Gurobi logs\n",
    "env.start()\n",
    "\n",
    "# Initialize the Gurobi model with suppressed logs\n",
    "model = Model(\"Rational_Function_Optimization\", env=env)\n",
    "multi_indices = generate_multi_indices(n_components, d)\n",
    "num_coefficients = len(multi_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab442fc3-6111-470b-981c-0a0dcf057d14",
   "metadata": {},
   "source": [
    "üåøFeasibility Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed6bf7f-e4ce-43fe-9074-e5c7c67f7ced",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from gurobipy import Model, GRB, quicksum\n",
    "import numpy as np\n",
    "\n",
    "def check_feasibility(z, x_train_norm, y_binary, G, H, num_coefficients):\n",
    "\n",
    "    delta = 1e-6  # Threshold for positivity constraint\n",
    "    n_samples = x_train_norm.shape[0]\n",
    "\n",
    "    # Initialize Gurobi model\n",
    "    model = Model(\"constraints\")\n",
    "    model.setParam(\"OutputFlag\", 0)  # Suppress Gurobi logs\n",
    "    # model.setParam(\"Seed\", 42)  # Fix solver seed for consistency\n",
    "    # model.setParam(\"Threads\", 1)  # Disable multi-threading for consistency\n",
    "\n",
    "    # Define variables\n",
    "    alpha = model.addVars(num_coefficients, lb=-GRB.INFINITY, name=\"alpha\")\n",
    "    beta = model.addVars(num_coefficients, lb=-GRB.INFINITY, name=\"beta\")\n",
    "    theta = model.addVar(lb=0, name=\"theta\")\n",
    "    intermediate_vars = model.addVars(n_samples, 3, lb=-GRB.INFINITY, name=\"intermediate\")\n",
    "\n",
    "    # Add constraints for each sample\n",
    "    for i in range(n_samples):\n",
    "        # Auxiliary variables for linearization\n",
    "        G_x = quicksum(alpha[j] * G[i, j] for j in range(num_coefficients))  # Œ±·µÄG(x·µ¢)\n",
    "        H_x = quicksum(beta[j] * H[i, j] for j in range(num_coefficients))   # Œ≤·µÄH(x·µ¢)\n",
    "        f_x = y_binary[i]  # Binary label for the sample\n",
    "\n",
    "        # Define intermediate variables to simplify nonlinear terms\n",
    "        model.addConstr(intermediate_vars[i, 0] == (f_x - z) * H_x, name=f\"term1_sample_{i}\")  # (f(x·µ¢) - z)¬∑Œ≤·µÄH(x·µ¢)\n",
    "        model.addConstr(intermediate_vars[i, 1] == (-(f_x + z)) * H_x, name=f\"term2_sample_{i}\")  # (-(f(x·µ¢) + z))¬∑Œ≤·µÄH(x·µ¢)\n",
    "        model.addConstr(intermediate_vars[i, 2] == H_x, name=f\"positivity_term_sample_{i}\")  # Œ≤·µÄH(x·µ¢)\n",
    "\n",
    "        # Constraint 1: (f(x·µ¢) - z)¬∑Œ≤·µÄH(x·µ¢) - Œ±·µÄG(x·µ¢) ‚â§ Œ∏\n",
    "        model.addConstr(intermediate_vars[i, 0] - G_x <= theta, name=f\"upper_bound_sample_{i}\")\n",
    "\n",
    "        # Constraint 2: Œ±·µÄG(x·µ¢) + (-(f(x·µ¢) + z))¬∑Œ≤·µÄH(x·µ¢) ‚â§ Œ∏\n",
    "        model.addConstr(G_x + intermediate_vars[i, 1] <= theta, name=f\"lower_bound_sample_{i}\")\n",
    "\n",
    "        # Constraint 3: Œ≤·µÄH(x·µ¢) ‚â• Œ¥\n",
    "        model.addConstr(intermediate_vars[i, 2] >= delta, name=f\"positivity_sample_{i}\")\n",
    "\n",
    "    # Set objective\n",
    "    model.setObjective(theta, GRB.MINIMIZE)\n",
    "\n",
    "    # Solve the model\n",
    "    model.optimize()\n",
    "\n",
    "    # Extract results\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        optimal_alpha = [alpha[j].X for j in range(num_coefficients)]\n",
    "        optimal_beta = [beta[j].X for j in range(num_coefficients)]\n",
    "        optimal_theta = theta.X\n",
    "        # print(f\"Optimal solution found: Theta = {optimal_theta}\")\n",
    "        return True, optimal_alpha, optimal_beta, optimal_theta\n",
    "    else:\n",
    "        print(f\"Model not feasible or no solution found. Status: {model.status}\")\n",
    "        return False, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad48621-d14a-4753-87ad-09f31fc10d32",
   "metadata": {},
   "source": [
    "üåøBisection Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16575e5c-d294-4e6c-8c60-0fb3d40f7609",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def bisection_loop(x_train_norm, y_binary, uL, uH, precision, model, num_coefficients, G, H, delta):\n",
    "    z_values = []\n",
    "    optimal_alpha, optimal_beta, optimal_theta = None, None, None\n",
    "    \n",
    "    # print(\"Starting bisection loop...\")\n",
    "    # print(f\"Initial bounds: uL={uL}, uH={uH}, precision={precision}\")\n",
    "\n",
    "    while uH - uL > precision:\n",
    "        z = (uL + uH) / 2  # Midpoint of bounds\n",
    "        z_values.append(z)\n",
    "        # print(f\"Testing z = {z}...\")\n",
    "\n",
    "        # Feasibility Check\n",
    "        feasible, alpha_coefficients, beta_coefficients, theta = check_feasibility(\n",
    "            z, x_train_norm, y_binary, G, H, num_coefficients\n",
    "        )\n",
    "\n",
    "        if feasible:\n",
    "            # print(f\"z = {z} is feasible.\")\n",
    "            uH = z\n",
    "            optimal_alpha, optimal_beta, optimal_theta = alpha_coefficients, beta_coefficients, theta\n",
    "        else:\n",
    "            # print(f\"z = {z} is not feasible.\")\n",
    "            uL = z\n",
    "\n",
    "    # print(\"Bisection loop completed.\")\n",
    "    # print(f\"Optimal z: {uH}\")\n",
    "    # print(f\"Optimal theta: {optimal_theta}\")\n",
    "    return uH, optimal_alpha, optimal_beta, optimal_theta, z_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8c11e-db2e-47b7-821e-0979fc9bb17f",
   "metadata": {},
   "source": [
    "üåøTrain (one-vs-all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0181ca8-3220-4950-b405-4e2caf318a5f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "# import pickle\n",
    "# from gurobipy import *\n",
    "\n",
    "# # import sys\n",
    "# # # Redirect stdout to a log file\n",
    "# # sys.stdout = open('gurobi_output.log', 'w')\n",
    "\n",
    "# #------------------------------------------------------------------------\n",
    "# # Binarize the labels for one-vs-all classification\n",
    "# lb = LabelBinarizer()\n",
    "# y_binarized = lb.fit_transform(y_train_subset)\n",
    "# # print(f\"y_binarized shape: {y_binarized.shape}\")\n",
    "\n",
    "# # Define bisection parameters\n",
    "# uL = 0.0  # Lower bound\n",
    "# uH = 10.0  # Upper bound\n",
    "# precision = 1e-8  # Precision for bisection loop\n",
    "# delta = 1e-6  # Threshold for positivity constraint\n",
    "\n",
    "# #------------------------------------------------------------------------\n",
    "# # Train models for each digit\n",
    "# for digit in range(10):\n",
    "#     print(f\"Training classifier for digit {digit}...\")\n",
    "\n",
    "#     # # Extract binary labels for the current digit (one-vs-all)\n",
    "#     # y_binary = y_binarized[:, digit]\n",
    "#     # Assign -1 to the current digit & 1 to others\n",
    "#     # y_binary = np.where(y_binarized[:, digit] == 1, -1, 1)\n",
    "\n",
    "#     y_binary = np.where(y_binarized[:, digit] == 1, 1, -1)\n",
    "\n",
    "#     # Construct G and H matrices for the training data\n",
    "#     G, H, multi_indices = construct_G_H_matrices(x_train_norm, n_components, d)\n",
    "\n",
    "#     # Initialize the Gurobi model\n",
    "#     model = Model(f\"digit_{digit}_classifier\")\n",
    "#     model.setParam('OutputFlag', 0)  # Suppress output during training\n",
    "\n",
    "#     # Run bisection loop to find optimal coefficients\n",
    "#     optimal_z, optimal_alpha, optimal_beta, optimal_theta, z_values = bisection_loop(\n",
    "#         x_train_norm, y_binary, uL, uH, precision, model, len(multi_indices), G, H, delta\n",
    "#     )\n",
    "\n",
    "#     # Check if a feasible solution was found\n",
    "#     if optimal_alpha is None or optimal_beta is None or optimal_theta is None:\n",
    "#         print(f\"No feasible solution found for digit {digit}. Skipping...\")\n",
    "#         continue\n",
    "\n",
    "#     #------------------------------------------------------------------------\n",
    "#     # Save the model parameters\n",
    "#     model_data = {\n",
    "#         \"alpha\": optimal_alpha,\n",
    "#         \"beta\": optimal_beta,\n",
    "#         \"theta\": optimal_theta,\n",
    "#         \"z\": optimal_z,\n",
    "#         \"n_components\": n_components,\n",
    "#         \"degree\": d,\n",
    "#         \"multi_indices\": multi_indices\n",
    "#     }\n",
    "#     with open(f\"{models_dir}/classifier_{digit}.pkl\", \"wb\") as file:\n",
    "#         pickle.dump(model_data, file)\n",
    "\n",
    "#     print(f\"Model for digit {digit} saved successfully!\")\n",
    "\n",
    "# #Summary\n",
    "# print(\"Training complete. Models saved in:\", models_dir)\n",
    "\n",
    "# # Stop timer\n",
    "# training_time = time.time() - start_time\n",
    "# print(f\"Total Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "# # # Restore stdout\n",
    "# # sys.stdout.close()\n",
    "# # sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0c148-da18-4ec2-8264-e0a17ae47c31",
   "metadata": {},
   "source": [
    "üåøTrain (one-vs-all) - multiprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd8009-4eda-40d4-af0f-d61c428060e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 cores for parallel training.\n",
      "Set parameter Username\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "from gurobipy import *\n",
    "\n",
    "# Function to train a single classifier for one digit\n",
    "def train_classifier(digit):\n",
    "    # print(f\"Training classifier for digit {digit}...\")\n",
    "\n",
    "    # Assign -1 to the current digit & 1 to others\n",
    "    y_binary = np.where(y_binarized[:, digit] == 1, 1, -1)\n",
    "\n",
    "    # Construct G and H matrices for the training data\n",
    "    G, H, multi_indices = construct_G_H_matrices(x_train_norm, n_components, d)\n",
    "\n",
    "    # Initialize the Gurobi model\n",
    "    model = Model(f\"digit_{digit}_classifier\")\n",
    "    model.setParam('OutputFlag', 0)  # Suppress Gurobi logs\n",
    "    model.setParam('Threads', 1)  # Restrict Gurobi to a single thread for parallel processes\n",
    "\n",
    "    # Run bisection loop to find optimal coefficients\n",
    "    optimal_z, optimal_alpha, optimal_beta, optimal_theta, z_values = bisection_loop(\n",
    "        x_train_norm, y_binary, uL, uH, precision, model, len(multi_indices), G, H, delta\n",
    "    )\n",
    "\n",
    "    # Check if a feasible solution was found\n",
    "    if optimal_alpha is None or optimal_beta is None or optimal_theta is None:\n",
    "        print(f\"No feasible solution found for digit {digit}. Skipping...\")\n",
    "        return None\n",
    "\n",
    "    # Save the model parameters\n",
    "    model_data = {\n",
    "        \"alpha\": optimal_alpha,\n",
    "        \"beta\": optimal_beta,\n",
    "        \"theta\": optimal_theta,\n",
    "        \"z\": optimal_z,\n",
    "        \"n_components\": n_components,\n",
    "        \"degree\": d,\n",
    "        \"multi_indices\": multi_indices\n",
    "    }\n",
    "    model_path = f\"{models_dir}/classifier_{digit}.pkl\"\n",
    "    with open(model_path, \"wb\") as file:\n",
    "        pickle.dump(model_data, file)\n",
    "\n",
    "    # Calculate individual training accuracy for this classifier\n",
    "    predictions = []\n",
    "    for i in range(len(x_train_norm)):\n",
    "        numerator = np.dot(G[i], optimal_alpha)\n",
    "        denominator = np.dot(H[i], optimal_beta)\n",
    "        rational_value = numerator / denominator if denominator != 0 else 1e-8\n",
    "        predictions.append(1 if rational_value > 0 else -1)  # Threshold at 0 for prediction\n",
    "\n",
    "    individual_accuracy = accuracy_score(y_binary, predictions) * 100\n",
    "    print(f\"Training Accuracy for Digit {digit}: {individual_accuracy:.2f}%\")\n",
    "\n",
    "    # print(f\"Model for digit {digit} saved successfully!\")\n",
    "    return digit, individual_accuracy\n",
    "\n",
    "# Main logic for parallel training\n",
    "if __name__ == \"__main__\":\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Binarize the labels for one-vs-all classification\n",
    "    lb = LabelBinarizer()\n",
    "    y_binarized = lb.fit_transform(y_train_subset)\n",
    "    # print(f\"y_binarized shape: {y_binarized.shape}\")\n",
    "\n",
    "    # Define bisection parameters\n",
    "    uL = 0.0  # Lower bound\n",
    "    uH = 10.0  # Upper bound\n",
    "    precision = 1e-8  # Precision for bisection loop\n",
    "    delta = 1e-6  # Threshold for positivity constraint\n",
    "\n",
    "    # Number of cores to use for parallel training\n",
    "    num_cores = min(cpu_count(), 10)  # Adjust to your 32-core processor\n",
    "    print(f\"Using {num_cores} cores for parallel training.\")\n",
    "\n",
    "    # Train classifiers for digits (0‚Äì9) in parallel\n",
    "    with Pool(num_cores) as pool:\n",
    "        results = pool.map(train_classifier, range(10))\n",
    "\n",
    "    # Stop timer\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Total Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    # Extract individual accuracies and calculate overall training accuracy\n",
    "    individual_accuracies = [result[1] for result in results if result is not None]\n",
    "    overall_predictions = []\n",
    "    for i in range(len(x_train_norm)):\n",
    "        confidence_scores = []\n",
    "        for digit in range(10):\n",
    "            # Load the model for the current digit\n",
    "            model_path = f\"{models_dir}/classifier_{digit}.pkl\"\n",
    "            with open(model_path, \"rb\") as file:\n",
    "                model = pickle.load(file)\n",
    "            alpha = model[\"alpha\"]\n",
    "            beta = model[\"beta\"]\n",
    "            multi_indices = model[\"multi_indices\"]\n",
    "            G, H, _ = construct_G_H_matrices(np.array([x_train_norm[i]]), model[\"n_components\"], model[\"degree\"])\n",
    "            numerator = np.dot(G[0], alpha)\n",
    "            denominator = np.dot(H[0], beta)\n",
    "            rational_value = numerator / denominator if denominator != 0 else 1e-8\n",
    "            confidence_scores.append(rational_value)\n",
    "        # Predict the digit with the highest confidence\n",
    "        overall_predictions.append(np.argmax(confidence_scores))\n",
    "\n",
    "    overall_training_accuracy = accuracy_score(y_train_subset, overall_predictions) * 100\n",
    "\n",
    "    # Print individual and overall accuracies\n",
    "    print(f\"Overall Training Accuracy: {overall_training_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954a7ce-bccf-4c03-8e62-9a71fc192fb9",
   "metadata": {},
   "source": [
    "üåøVisual Cross-verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba3368-91a9-498f-b336-09783d66c60b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Specify the range of indices to visualize\n",
    "start_index = 150\n",
    "end_index = 200\n",
    "\n",
    "# Number of rows and columns for the grid\n",
    "rows = int(np.ceil((end_index - start_index) / 5))  # 5 images per row\n",
    "cols = 5\n",
    "\n",
    "# Create a figure\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
    "\n",
    "# Flatten axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through the specified range\n",
    "for idx, ax in zip(range(start_index, end_index), axes):\n",
    "    # Predicted and true labels for the current index\n",
    "    predicted_digit = overall_predictions[idx]\n",
    "    true_digit = y_train_subset[idx]\n",
    "\n",
    "    # Reshape the flattened image back to its original shape (28x28)\n",
    "    image = x_train_subset[idx].reshape(28, 28)\n",
    "\n",
    "    # Determine correctness\n",
    "    if predicted_digit == true_digit:\n",
    "        status = \"correct‚úîÔ∏è\"\n",
    "    else:\n",
    "        status = \"wrong‚úñÔ∏è\"\n",
    "    \n",
    "    # Plot the image\n",
    "    ax.imshow(image, cmap='gray')  # Use 'gray' colormap for MNIST\n",
    "    ax.set_title(f\"Pred: {predicted_digit}, True: {true_digit}\\n{status}\")\n",
    "    ax.axis(\"off\")  # Hide axes for cleaner visuals\n",
    "\n",
    "# Turn off unused subplots\n",
    "for ax in axes[len(range(start_index, end_index)):]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c2c6e-6282-4b7f-bf3d-05a3ab8b18a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SageMath)",
   "language": "python",
   "name": "sagemath-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
