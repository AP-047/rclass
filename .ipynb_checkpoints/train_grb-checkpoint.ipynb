{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad4e77f-a086-42c9-93aa-090d26a5032c",
   "metadata": {},
   "source": [
    "ðŸŒ¿Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e6b5cf5-f600-4a1c-bda8-2451479d6075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28), y_train shape: (60000,)\n",
      "1. x_train_flat shape: (60000, 784), y_train shape: (60000,)\n",
      "2. x_train_subset shape: (7000, 784)\n",
      "3. x_train_pca shape: (7000, 20)\n",
      "variance = 0.6530388842286752\n",
      "(train pca model saved)\n",
      "4. x_train_norm shape: (7000, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "# load data\n",
    "mnist_path = \"/home/ajay2425/rclass/mnist_dataset/mnist.npz\"\n",
    "with np.load(mnist_path) as data:\n",
    "    x_train = data[\"x_train\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "# 1. Flatten\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
    "print(f\"1. x_train_flat shape: {x_train_flat.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "# 2. Subsets\n",
    "subset_size = 7000\n",
    "x_train_subset = x_train_flat[:subset_size]\n",
    "y_train_subset = y_train[:subset_size]\n",
    "print(f\"2. x_train_subset shape: {x_train_subset.shape}\")\n",
    "\n",
    "# 3. PCA\n",
    "n_components = 20\n",
    "d = 2 # degree\n",
    "pca = PCA(n_components=n_components)\n",
    "x_train_pca = pca.fit_transform(x_train_subset)\n",
    "print(f\"3. x_train_pca shape: {x_train_pca.shape}\")\n",
    "variance = np.sum(pca.explained_variance_ratio_)\n",
    "print(f\"variance = {variance}\")\n",
    "\n",
    "# Save the trained PCA model\n",
    "pca_model_path = \"/home/ajay2425/rclass/models/models_grb/trained_pca.pkl\" # main\n",
    "# pca_model_path = \"/home/ajay2425/rclass/models/add/trained_pca.pkl\" # temp.\n",
    "with open(pca_model_path, \"wb\") as file:\n",
    "    pickle.dump(pca, file)\n",
    "print(f\"(train pca model saved)\")\n",
    "\n",
    "\n",
    "# 4. Normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_train_norm = scaler.fit_transform(x_train_pca)\n",
    "print(f\"4. x_train_norm shape: {x_train_norm.shape}\")\n",
    "\n",
    "# # 4. Binarize\n",
    "# threshold_value = 0\n",
    "# x_train_norm = (x_train_pca > threshold_value).astype(int)\n",
    "# print(f\"4. x_train_norm shape: {x_train_norm.shape}\")\n",
    "# # print(x_train_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b28b24-5624-4723-96e8-8e264b30629c",
   "metadata": {},
   "source": [
    "ðŸŒ¿Check Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82ebaf3-2131-4139-acfb-090eb7a74fb1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory: /home/ajay2425/rclass/models/models_grb/\n",
      "Directory already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the directory\n",
    "models_dir = \"/home/ajay2425/rclass/models/models_grb/\" # main\n",
    "# pca_model_path = \"/home/ajay2425/rclass/models/add/trained_pca.pkl\" # temp.\n",
    "print(f\"Using existing directory: {models_dir}\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(models_dir):\n",
    "    print(\"Directory already exists.\")\n",
    "else:\n",
    "    print(\"Directory does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e26809-7a30-41b2-b04e-c174f9caf870",
   "metadata": {},
   "source": [
    "ðŸŒ¿Generate multi-Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6f3d3b-a38c-4aeb-9ff3-fcdb709b203a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_of_coeff = 231\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "def r_multi_indices(n, d):\n",
    "    if n == 1:\n",
    "        yield (d,)\n",
    "    else:\n",
    "        for k in range(d + 1):\n",
    "            for c in r_multi_indices(n - 1, k):\n",
    "                yield (d - k, *c)\n",
    "\n",
    "def generate_multi_indices(n, d):\n",
    "    from itertools import chain\n",
    "    return list(chain(*[list(r_multi_indices(n, _)) for _ in range(d + 1)]))\n",
    "\n",
    "c = generate_multi_indices(n_components, d)\n",
    "print(f\"no_of_coeff =\", len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bff7d1-259a-4542-9616-54c44cdbb37b",
   "metadata": {},
   "source": [
    "ðŸŒ¿Generate Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599aa024-5b41-4081-ae03-f9e24537c783",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def construct_G_H_matrices(x_train_norm, n, d):\n",
    "    num_data_points = x_train_norm.shape[0]\n",
    "    multi_indices = generate_multi_indices(n, d)\n",
    "    num_coefficients = len(multi_indices)\n",
    "\n",
    "    # Initialize G and H matrices\n",
    "    G = []\n",
    "    H = []\n",
    "\n",
    "    # Construct G and H using multi-indices\n",
    "    for i in range(num_data_points):\n",
    "        G_row = []\n",
    "        H_row = []\n",
    "        for idx in multi_indices:\n",
    "            term = np.prod([x_train_norm[i, k] ** idx[k] for k in range(n)])\n",
    "            G_row.append(term)\n",
    "            H_row.append(term)\n",
    "        G.append(G_row)\n",
    "        H.append(H_row)\n",
    "\n",
    "    # Convert G and H to NumPy arrays\n",
    "    G = np.array(G)\n",
    "    H = np.array(H)\n",
    "\n",
    "    # # Normalize G and H row-wise for numerical stability\n",
    "    # G = G / np.linalg.norm(G, axis=1, keepdims=True)\n",
    "    # H = H / np.linalg.norm(H, axis=1, keepdims=True)\n",
    "\n",
    "    return G, H, multi_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adc6f1-2cf6-40a2-ac7a-7e84932e7451",
   "metadata": {},
   "source": [
    "ðŸŒ¿Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32e4983-421b-434b-a3e1-6a6f88717fc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from gurobipy import Model, GRB, Env\n",
    "\n",
    "# Suppress Gurobi logs globally\n",
    "env = Env(empty=True)\n",
    "env.setParam(\"OutputFlag\", 0)  # Suppress Gurobi logs\n",
    "env.start()\n",
    "\n",
    "# Initialize the Gurobi model with suppressed logs\n",
    "model = Model(\"Rational_Function_Optimization\", env=env)\n",
    "multi_indices = generate_multi_indices(n_components, d)\n",
    "num_coefficients = len(multi_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab442fc3-6111-470b-981c-0a0dcf057d14",
   "metadata": {},
   "source": [
    "ðŸŒ¿Feasibility Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed6bf7f-e4ce-43fe-9074-e5c7c67f7ced",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from gurobipy import Model, GRB, quicksum\n",
    "import numpy as np\n",
    "\n",
    "def check_feasibility(z, x_train_norm, y_binary, G, H, num_coefficients):\n",
    "\n",
    "    delta = 1e-6  # Threshold for positivity constraint\n",
    "    n_samples = x_train_norm.shape[0]\n",
    "\n",
    "    # Initialize Gurobi model\n",
    "    model = Model(\"constraints\")\n",
    "    model.setParam(\"OutputFlag\", 0)  # Suppress Gurobi logs\n",
    "    # model.setParam(\"Seed\", 42)  # Fix solver seed for consistency\n",
    "    # model.setParam(\"Threads\", 1)  # Disable multi-threading for consistency\n",
    "\n",
    "    # Define variables\n",
    "    alpha = model.addVars(num_coefficients, lb=-GRB.INFINITY, name=\"alpha\")\n",
    "    beta = model.addVars(num_coefficients, lb=-GRB.INFINITY, name=\"beta\")\n",
    "    theta = model.addVar(lb=0, name=\"theta\")\n",
    "    intermediate_vars = model.addVars(n_samples, 3, lb=-GRB.INFINITY, name=\"intermediate\")\n",
    "\n",
    "    # Add constraints for each sample\n",
    "    for i in range(n_samples):\n",
    "        # Auxiliary variables for linearization\n",
    "        G_x = quicksum(alpha[j] * G[i, j] for j in range(num_coefficients))  # Î±áµ€G(xáµ¢)\n",
    "        H_x = quicksum(beta[j] * H[i, j] for j in range(num_coefficients))   # Î²áµ€H(xáµ¢)\n",
    "        f_x = y_binary[i]  # Binary label for the sample\n",
    "\n",
    "        # Define intermediate variables to simplify nonlinear terms\n",
    "        model.addConstr(intermediate_vars[i, 0] == (f_x - z) * H_x, name=f\"term1_sample_{i}\")  # (f(xáµ¢) - z)Â·Î²áµ€H(xáµ¢)\n",
    "        model.addConstr(intermediate_vars[i, 1] == (-(f_x + z)) * H_x, name=f\"term2_sample_{i}\")  # (-(f(xáµ¢) + z))Â·Î²áµ€H(xáµ¢)\n",
    "        model.addConstr(intermediate_vars[i, 2] == H_x, name=f\"positivity_term_sample_{i}\")  # Î²áµ€H(xáµ¢)\n",
    "\n",
    "        # Constraint 1: (f(xáµ¢) - z)Â·Î²áµ€H(xáµ¢) - Î±áµ€G(xáµ¢) â‰¤ Î¸\n",
    "        model.addConstr(intermediate_vars[i, 0] - G_x <= theta, name=f\"upper_bound_sample_{i}\")\n",
    "\n",
    "        # Constraint 2: Î±áµ€G(xáµ¢) + (-(f(xáµ¢) + z))Â·Î²áµ€H(xáµ¢) â‰¤ Î¸\n",
    "        model.addConstr(G_x + intermediate_vars[i, 1] <= theta, name=f\"lower_bound_sample_{i}\")\n",
    "\n",
    "        # Constraint 3: Î²áµ€H(xáµ¢) â‰¥ Î´\n",
    "        model.addConstr(intermediate_vars[i, 2] >= delta, name=f\"positivity_sample_{i}\")\n",
    "\n",
    "    # Set objective\n",
    "    model.setObjective(theta, GRB.MINIMIZE)\n",
    "\n",
    "    # Solve the model\n",
    "    model.optimize()\n",
    "\n",
    "    # Extract results\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        optimal_alpha = [alpha[j].X for j in range(num_coefficients)]\n",
    "        optimal_beta = [beta[j].X for j in range(num_coefficients)]\n",
    "        optimal_theta = theta.X\n",
    "        print(f\"Optimal solution found: Theta = {optimal_theta}\")\n",
    "        return True, optimal_alpha, optimal_beta, optimal_theta\n",
    "    else:\n",
    "        print(f\"Model not feasible or no solution found. Status: {model.status}\")\n",
    "        return False, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad48621-d14a-4753-87ad-09f31fc10d32",
   "metadata": {},
   "source": [
    "ðŸŒ¿Bisection Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16575e5c-d294-4e6c-8c60-0fb3d40f7609",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def bisection_loop(x_train_norm, y_binary, uL, uH, precision, model, num_coefficients, G, H, delta):\n",
    "    z_values = []\n",
    "    optimal_alpha, optimal_beta, optimal_theta = None, None, None\n",
    "    \n",
    "    print(\"Starting bisection loop...\")\n",
    "    print(f\"Initial bounds: uL={uL}, uH={uH}, precision={precision}\")\n",
    "\n",
    "    while uH - uL > precision:\n",
    "        z = (uL + uH) / 2  # Midpoint of bounds\n",
    "        z_values.append(z)\n",
    "        print(f\"Testing z = {z}...\")\n",
    "\n",
    "        # Feasibility Check\n",
    "        feasible, alpha_coefficients, beta_coefficients, theta = check_feasibility(\n",
    "            z, x_train_norm, y_binary, G, H, num_coefficients\n",
    "        )\n",
    "\n",
    "        # for Debugging feasibility results\n",
    "        if feasible:\n",
    "            print(f\"z = {z} is feasible.\")\n",
    "            uH = z\n",
    "            optimal_alpha, optimal_beta, optimal_theta = alpha_coefficients, beta_coefficients, theta\n",
    "        else:\n",
    "            print(f\"z = {z} is not feasible.\")\n",
    "            uL = z\n",
    "\n",
    "    print(\"Bisection loop completed.\")\n",
    "    print(f\"Optimal z: {uH}\")\n",
    "    print(f\"Optimal theta: {optimal_theta}\")\n",
    "    return uH, optimal_alpha, optimal_beta, optimal_theta, z_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8c11e-db2e-47b7-821e-0979fc9bb17f",
   "metadata": {},
   "source": [
    "ðŸŒ¿Train (one-vs-all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e86528-eefd-4257-ad3f-d378087f1677",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier for digit 0...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "from gurobipy import *\n",
    "\n",
    "# import sys\n",
    "# # Redirect stdout to a log file\n",
    "# sys.stdout = open('gurobi_output.log', 'w')\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# Binarize the labels for one-vs-all classification\n",
    "lb = LabelBinarizer()\n",
    "y_binarized = lb.fit_transform(y_train_subset)\n",
    "# print(f\"y_binarized shape: {y_binarized.shape}\")\n",
    "\n",
    "# Define bisection parameters\n",
    "uL = 0.0  # Lower bound\n",
    "uH = 100.0  # Upper bound\n",
    "precision = 1e-6  # Precision for bisection loop\n",
    "delta = 1e-6  # Threshold for positivity constraint\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# Train models for each digit\n",
    "for digit in range(10):\n",
    "    print(f\"Training classifier for digit {digit}...\")\n",
    "\n",
    "    # # Extract binary labels for the current digit (one-vs-all)\n",
    "    # y_binary = y_binarized[:, digit]\n",
    "    # Assign -1 to the current digit & 1 to others\n",
    "    # y_binary = np.where(y_binarized[:, digit] == 1, -1, 1)\n",
    "\n",
    "    y_binary = np.where(y_binarized[:, digit] == 1, 1, -1)\n",
    "\n",
    "    # Construct G and H matrices for the training data\n",
    "    G, H, multi_indices = construct_G_H_matrices(x_train_norm, n_components, d)\n",
    "\n",
    "    # Initialize the Gurobi model\n",
    "    model = Model(f\"digit_{digit}_classifier\")\n",
    "    model.setParam('OutputFlag', 0)  # Suppress output during training\n",
    "\n",
    "    # Run bisection loop to find optimal coefficients\n",
    "    optimal_z, optimal_alpha, optimal_beta, optimal_theta, z_values = bisection_loop(\n",
    "        x_train_norm, y_binary, uL, uH, precision, model, len(multi_indices), G, H, delta\n",
    "    )\n",
    "\n",
    "    # Check if a feasible solution was found\n",
    "    if optimal_alpha is None or optimal_beta is None or optimal_theta is None:\n",
    "        print(f\"No feasible solution found for digit {digit}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    # Save the model parameters\n",
    "    model_data = {\n",
    "        \"alpha\": optimal_alpha,\n",
    "        \"beta\": optimal_beta,\n",
    "        \"theta\": optimal_theta,\n",
    "        \"z\": optimal_z,\n",
    "        \"n_components\": n_components,\n",
    "        \"degree\": d,\n",
    "        \"multi_indices\": multi_indices\n",
    "    }\n",
    "    with open(f\"{models_dir}/classifier_{digit}.pkl\", \"wb\") as file:\n",
    "        pickle.dump(model_data, file)\n",
    "\n",
    "    print(f\"Model for digit {digit} saved successfully!\")\n",
    "\n",
    "#Summary\n",
    "print(\"Training complete. Models saved in:\", models_dir)\n",
    "\n",
    "# Stop timer\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Total Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "# # Restore stdout\n",
    "# sys.stdout.close()\n",
    "# sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd8009-4eda-40d4-af0f-d61c428060e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SageMath)",
   "language": "python",
   "name": "sagemath-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
