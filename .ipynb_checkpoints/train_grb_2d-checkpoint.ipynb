{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a24d934f-c062-45a0-94df-db1e6a45ab64",
   "metadata": {},
   "source": [
    "🌿Set Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "055b6c0e-e5c0-489c-ac79-91f3f334992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size_per_digit = 2000\n",
    "n_components = 25\n",
    "degree_n = 2\n",
    "degree_d = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c6c7af-2e83-40f9-95dd-d5aa1d8e30df",
   "metadata": {},
   "source": [
    "🌿Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e6b5cf5-f600-4a1c-bda8-2451479d6075",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28), y_train shape: (60000,)\n",
      "1. x_train_flat shape: (60000, 784), y_train shape: (60000,)\n",
      "2. x_train_subset shape: (20000, 784)\n",
      "3. x_train_pca shape: (20000, 25)\n",
      "variance = 69.07285154439845%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "# load local mnist data file\n",
    "mnist_path = \"/home/ajay2425/rclass/mnist_dataset/mnist.npz\"\n",
    "with np.load(mnist_path) as data:\n",
    "    x_train = data[\"x_train\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "# 1. Flatten\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
    "print(f\"1. x_train_flat shape: {x_train_flat.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "# 2. Subsets (with equal distribution)\n",
    "# subset_size_per_digit = 350\n",
    "unique_digits = np.unique(y_train)\n",
    "\n",
    "x_train_balanced = []\n",
    "y_train_balanced = []\n",
    "\n",
    "for digit in unique_digits:\n",
    "    # Get indices of all samples of the current digit\n",
    "    digit_indices = np.where(y_train == digit)[0]\n",
    "    # Randomly sample subset_size_per_digit indices\n",
    "    sampled_indices = np.random.choice(digit_indices, subset_size_per_digit, replace=False)\n",
    "    # Append the sampled data\n",
    "    x_train_balanced.append(x_train_flat[sampled_indices])\n",
    "    y_train_balanced.append(y_train[sampled_indices])\n",
    "\n",
    "# Combine and shuffle the balanced dataset\n",
    "x_train_balanced = np.vstack(x_train_balanced)\n",
    "y_train_balanced = np.hstack(y_train_balanced)\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffled_indices = np.random.permutation(len(y_train_balanced))\n",
    "x_train_subset = x_train_balanced[shuffled_indices]\n",
    "y_train_subset = y_train_balanced[shuffled_indices]\n",
    "print(f\"2. x_train_subset shape: {x_train_subset.shape}\")\n",
    "\n",
    "# 3. PCA\n",
    "# n_components = 19\n",
    "# d = 2 # degree\n",
    "pca = PCA(n_components=n_components)\n",
    "x_train_pca = pca.fit_transform(x_train_subset)\n",
    "print(f\"3. x_train_pca shape: {x_train_pca.shape}\")\n",
    "variance = np.sum(pca.explained_variance_ratio_)\n",
    "print(f\"variance = {variance*100}%\")\n",
    "\n",
    "# Save the trained PCA model\n",
    "pca_model_path = \"/home/ajay2425/rclass/models/models_grb/trained_pca.pkl\"\n",
    "with open(pca_model_path, \"wb\") as file:\n",
    "    pickle.dump(pca, file)\n",
    "\n",
    "x_train_norm = x_train_pca\n",
    "\n",
    "# # 4. Normalize\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# x_train_norm = scaler.fit_transform(x_train_pca)\n",
    "# print(f\"4. x_train_norm shape: {x_train_norm.shape}\")\n",
    "\n",
    "# # 4. Binarize\n",
    "# threshold_value = 0\n",
    "# x_train_norm = (x_train_pca > threshold_value).astype(int)\n",
    "# print(f\"4. x_train_norm shape: {x_train_norm.shape}\")\n",
    "# # print(x_train_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b28b24-5624-4723-96e8-8e264b30629c",
   "metadata": {},
   "source": [
    "🌿Check Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82ebaf3-2131-4139-acfb-090eb7a74fb1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory: /home/ajay2425/rclass/models/models_grb/\n",
      "Directory already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the directory\n",
    "models_dir = \"/home/ajay2425/rclass/models/models_grb/\"\n",
    "print(f\"Using existing directory: {models_dir}\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(models_dir):\n",
    "    print(\"Directory already exists.\")\n",
    "else:\n",
    "    print(\"Directory does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e26809-7a30-41b2-b04e-c174f9caf870",
   "metadata": {},
   "source": [
    "🌿Generate multi-Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c6f3d3b-a38c-4aeb-9ff3-fcdb709b203a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of coeff in Numerator = 351\n",
      "no. of coeff in Numerator = 26\n",
      "Total no. of coeff = 377\n"
     ]
    }
   ],
   "source": [
    "def r_multi_indices(n, d):\n",
    "    if n == 1:\n",
    "        yield (d,)\n",
    "    else:\n",
    "        for k in range(d + 1):\n",
    "            for c in r_multi_indices(n - 1, k):\n",
    "                yield (d - k, *c)\n",
    "\n",
    "def generate_multi_indices(n, d):\n",
    "    from itertools import chain\n",
    "    return list(chain(*[list(r_multi_indices(n, _)) for _ in range(d + 1)]))\n",
    "\n",
    "mi_n = generate_multi_indices(n_components, degree_n)\n",
    "print(f\"no. of coeff in Numerator =\", len(mi_n))\n",
    "mi_d = generate_multi_indices(n_components, degree_d)\n",
    "print(f\"no. of coeff in Numerator =\", len(mi_d))\n",
    "mi_t = len(mi_n) + len(mi_d)\n",
    "print(f\"Total no. of coeff =\", mi_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bff7d1-259a-4542-9616-54c44cdbb37b",
   "metadata": {},
   "source": [
    "🌿Generate Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "599aa024-5b41-4081-ae03-f9e24537c783",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def construct_G_H_matrices(x_train_norm, n, degree_n, degree_d):\n",
    "    num_data_points = x_train_norm.shape[0]\n",
    "    multi_indices_n = generate_multi_indices(n, degree_n)\n",
    "    multi_indices_d = generate_multi_indices(n, degree_d)\n",
    "    num_coefficients_n = len(multi_indices_n)\n",
    "    num_coefficients_d = len(multi_indices_d)\n",
    "    \n",
    "    # Initialize G and H matrices\n",
    "    G = []\n",
    "    H = []\n",
    "\n",
    "    # Construct G using multi-indices\n",
    "    for i in range(num_data_points):\n",
    "        G_row = []\n",
    "        for idx in multi_indices_n:\n",
    "            term = np.prod([x_train_norm[i, k] ** idx[k] for k in range(n)])\n",
    "            G_row.append(term)\n",
    "        G.append(G_row)\n",
    "\n",
    "    # Construct H using multi-indices\n",
    "    for i in range(num_data_points):\n",
    "        H_row = []\n",
    "        for idx in multi_indices_d:\n",
    "            term = np.prod([x_train_norm[i, k] ** idx[k] for k in range(n)])\n",
    "            H_row.append(term)\n",
    "        H.append(H_row)\n",
    "\n",
    "    # Convert G and H to NumPy arrays\n",
    "    G = np.array(G)\n",
    "    H = np.array(H)\n",
    "\n",
    "    # Normalize G and H row-wise for numerical stability\n",
    "    G = G / np.linalg.norm(G, axis=1, keepdims=True)\n",
    "    H = H / np.linalg.norm(H, axis=1, keepdims=True)\n",
    "\n",
    "    return G, H, multi_indices_n, multi_indices_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adc6f1-2cf6-40a2-ac7a-7e84932e7451",
   "metadata": {},
   "source": [
    "🌿Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b32e4983-421b-434b-a3e1-6a6f88717fc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from gurobipy import Model, GRB, Env\n",
    "\n",
    "# Suppress Gurobi logs globally\n",
    "env = Env(empty=True)\n",
    "env.setParam(\"OutputFlag\", 0)  # Suppress Gurobi logs\n",
    "env.start()\n",
    "\n",
    "# Initialize the Gurobi model with suppressed logs\n",
    "model = Model(\"Rational_Function_Optimization\", env=env)\n",
    "multi_indices_n = generate_multi_indices(n_components, degree_n)\n",
    "multi_indices_d = generate_multi_indices(n_components, degree_d)\n",
    "num_coefficients_n = len(multi_indices_n)\n",
    "num_coefficients_d = len(multi_indices_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab442fc3-6111-470b-981c-0a0dcf057d14",
   "metadata": {},
   "source": [
    "🌿Feasibility Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eed6bf7f-e4ce-43fe-9074-e5c7c67f7ced",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from gurobipy import Model, GRB, quicksum\n",
    "import numpy as np\n",
    "\n",
    "def check_feasibility(z, x_train_norm, y_binary, G, H, num_coefficients_n, num_coefficients_d):\n",
    "\n",
    "    delta = 1e-6  # Threshold for positivity constraint\n",
    "    n_samples = x_train_norm.shape[0]\n",
    "\n",
    "    # Initialize Gurobi model\n",
    "    model = Model(\"constraints\")\n",
    "    model.setParam(\"OutputFlag\", 0)  # Suppress Gurobi logs\n",
    "    # model.setParam(\"Seed\", 42)  # Fix solver seed for consistency\n",
    "    # model.setParam(\"Threads\", 1)  # Disable multi-threading for consistency\n",
    "\n",
    "    # Define variables\n",
    "    alpha = model.addVars(num_coefficients_n, lb=-GRB.INFINITY, name=\"alpha\")\n",
    "    beta = model.addVars(num_coefficients_d, lb=-GRB.INFINITY, name=\"beta\")\n",
    "    theta = model.addVar(lb=0, name=\"theta\")\n",
    "    intermediate_vars = model.addVars(n_samples, 3, lb=-GRB.INFINITY, name=\"intermediate\")\n",
    "\n",
    "    # Add constraints for each sample\n",
    "    for i in range(n_samples):\n",
    "        # Auxiliary variables for linearization\n",
    "        G_x = quicksum(alpha[j] * G[i, j] for j in range(num_coefficients_n))  # αᵀG(xᵢ)\n",
    "        H_x = quicksum(beta[j] * H[i, j] for j in range(num_coefficients_d))   # βᵀH(xᵢ)\n",
    "        f_x = y_binary[i]  # Binary label for the sample\n",
    "\n",
    "        # Define intermediate variables to simplify nonlinear terms\n",
    "        model.addConstr(intermediate_vars[i, 0] == (f_x - z) * H_x, name=f\"term1_sample_{i}\")  # (f(xᵢ) - z)·βᵀH(xᵢ)\n",
    "        model.addConstr(intermediate_vars[i, 1] == (-(f_x + z)) * H_x, name=f\"term2_sample_{i}\")  # (-(f(xᵢ) + z))·βᵀH(xᵢ)\n",
    "        model.addConstr(intermediate_vars[i, 2] == H_x, name=f\"positivity_term_sample_{i}\")  # βᵀH(xᵢ)\n",
    "\n",
    "        # Constraint 1: (f(xᵢ) - z)·βᵀH(xᵢ) - αᵀG(xᵢ) ≤ θ\n",
    "        model.addConstr(intermediate_vars[i, 0] - G_x <= theta, name=f\"upper_bound_sample_{i}\")\n",
    "\n",
    "        # Constraint 2: αᵀG(xᵢ) + (-(f(xᵢ) + z))·βᵀH(xᵢ) ≤ θ\n",
    "        model.addConstr(G_x + intermediate_vars[i, 1] <= theta, name=f\"lower_bound_sample_{i}\")\n",
    "\n",
    "        # Constraint 3: βᵀH(xᵢ) ≥ δ\n",
    "        model.addConstr(intermediate_vars[i, 2] >= delta, name=f\"positivity_sample_{i}\")\n",
    "\n",
    "    # Set objective\n",
    "    model.setObjective(theta, GRB.MINIMIZE)\n",
    "\n",
    "    # Solve the model\n",
    "    model.optimize()\n",
    "\n",
    "    # Extract results\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        optimal_alpha = [alpha[j].X for j in range(num_coefficients_n)]\n",
    "        optimal_beta = [beta[j].X for j in range(num_coefficients_d)]\n",
    "        optimal_theta = theta.X\n",
    "        # print(f\"Optimal solution found: Theta = {optimal_theta}\")\n",
    "        return True, optimal_alpha, optimal_beta, optimal_theta\n",
    "    else:\n",
    "        # print(f\"Model not feasible or no solution found. Status: {model.status}\")\n",
    "        return False, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad48621-d14a-4753-87ad-09f31fc10d32",
   "metadata": {},
   "source": [
    "🌿Bisection Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16575e5c-d294-4e6c-8c60-0fb3d40f7609",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def bisection_loop(x_train_norm, y_binary, uL, uH, precision, model, num_coefficients_n, num_coefficients_d, G, H, delta):\n",
    "    z_values = []\n",
    "    optimal_alpha, optimal_beta, optimal_theta = None, None, None\n",
    "    \n",
    "    # print(\"Starting bisection loop...\")\n",
    "    # print(f\"Initial bounds: uL={uL}, uH={uH}, precision={precision}\")\n",
    "\n",
    "    while uH - uL > precision:\n",
    "        z = (uL + uH) / 2  # Midpoint of bounds\n",
    "        z_values.append(z)\n",
    "        # print(f\"Testing z = {z}...\")\n",
    "\n",
    "        # Feasibility Check\n",
    "        feasible, alpha_coefficients, beta_coefficients, theta = check_feasibility(\n",
    "            z, x_train_norm, y_binary, G, H, num_coefficients_n, num_coefficients_d\n",
    "        )\n",
    "\n",
    "        if feasible:\n",
    "            # print(f\"z = {z} is feasible.\")\n",
    "            uH = z\n",
    "            optimal_alpha, optimal_beta, optimal_theta = alpha_coefficients, beta_coefficients, theta\n",
    "        else:\n",
    "            # print(f\"z = {z} is not feasible.\")\n",
    "            uL = z\n",
    "\n",
    "    # print(\"Bisection loop completed.\")\n",
    "    # print(f\"Optimal z: {uH}\")\n",
    "    # print(f\"Optimal theta: {optimal_theta}\")\n",
    "    return uH, optimal_alpha, optimal_beta, optimal_theta, z_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0c148-da18-4ec2-8264-e0a17ae47c31",
   "metadata": {},
   "source": [
    "🌿Train (one-vs-all) - multiprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd8009-4eda-40d4-af0f-d61c428060e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 cores for parallel training\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-01-12\n",
      "Training Time: 3038.69 seconds\n",
      "Training Accuracy for Digit 0: 100.00%\n",
      "Training Accuracy for Digit 1: 99.98%\n",
      "Training Accuracy for Digit 2: 97.91%\n",
      "Training Accuracy for Digit 3: 97.79%\n",
      "Training Accuracy for Digit 4: 99.41%\n",
      "Training Accuracy for Digit 5: 98.84%\n",
      "Training Accuracy for Digit 6: 99.92%\n",
      "Training Accuracy for Digit 7: 98.57%\n",
      "Training Accuracy for Digit 8: 97.74%\n",
      "Training Accuracy for Digit 9: 97.28%\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "from gurobipy import *\n",
    "\n",
    "# Function to train a single classifier for one digit\n",
    "def train_classifier(digit):\n",
    "    # print(f\"Training classifier for digit {digit}...\")\n",
    "\n",
    "    # current digit=1, other digits=0\n",
    "    y_binary = np.where(y_binarized[:, digit] == 1, 1, 0)\n",
    "    # y_binary = np.where(y_binarized[:, digit] == 1, 0.1, -0.1)\n",
    "\n",
    "    # Construct G and H matrices for the training data\n",
    "    G, H, multi_indices_n, multi_indices_d = construct_G_H_matrices(x_train_norm, n_components, degree_n, degree_d)\n",
    "\n",
    "    # Initialize the Gurobi model\n",
    "    model = Model(f\"digit_{digit}_classifier\")\n",
    "    model.setParam('OutputFlag', 0)  # Suppress Gurobi logs\n",
    "    # model.setParam('Threads', 10)     # restricting each Gurobi instance to a single thread\n",
    "\n",
    "    # Run bisection loop to find optimal coefficients\n",
    "    optimal_z, optimal_alpha, optimal_beta, optimal_theta, z_values = bisection_loop(\n",
    "        x_train_norm, y_binary, uL, uH, precision, model, len(multi_indices_n), len(multi_indices_d), G, H, delta\n",
    "    )\n",
    "\n",
    "    # Check if a feasible solution was found\n",
    "    if optimal_alpha is None or optimal_beta is None or optimal_theta is None:\n",
    "        print(f\"No feasible solution found for digit {digit}. Skipping...\")\n",
    "        return None\n",
    "\n",
    "    # Save the model parameters\n",
    "    model_data = {\n",
    "        \"alpha\": optimal_alpha,\n",
    "        \"beta\": optimal_beta,\n",
    "        \"theta\": optimal_theta,\n",
    "        \"z\": optimal_z,\n",
    "        \"n_components\": n_components,\n",
    "        \"degree_n\": degree_n,\n",
    "        \"degree_d\": degree_d,\n",
    "        \"multi_indices_n\": multi_indices_n,\n",
    "        \"multi_indices_d\": multi_indices_d\n",
    "    }\n",
    "    model_path = f\"{models_dir}/classifier_{digit}.pkl\"\n",
    "    with open(model_path, \"wb\") as file:\n",
    "        pickle.dump(model_data, file)\n",
    "\n",
    "    return digit\n",
    "\n",
    "# training logic\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    # Binarize the labels for one-vs-all classification\n",
    "    lb = LabelBinarizer()\n",
    "    y_binarized = lb.fit_transform(y_train_subset)\n",
    "\n",
    "    # Define bisection parameters\n",
    "    uL = 0  # Lower bound\n",
    "    uH = 10  # Upper bound\n",
    "    precision = 1e-8  # Precision for bisection loop\n",
    "    delta = 1e-6  # Threshold for positivity constraint\n",
    "\n",
    "    # Number of cores to use for parallel training\n",
    "    num_cores = min(cpu_count(), 10)\n",
    "    print(f\"Using {num_cores} cores for parallel training\")\n",
    "\n",
    "    # Train classifiers for digits (0–9) in parallel\n",
    "    with Pool(num_cores) as pool:\n",
    "        results = pool.map(train_classifier, range(10))\n",
    "\n",
    "    # Stop timer for training\n",
    "    training_time = time.time() - training_start_time\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "    #------------------------------------------------------\n",
    "    # Calculate individual training accuracy for each classifier\n",
    "    individual_accuracies = []\n",
    "    for digit in range(10):\n",
    "        model_path = f\"{models_dir}/classifier_{digit}.pkl\"\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Model for digit {digit} not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Load the model for the current digit\n",
    "        with open(model_path, \"rb\") as file:\n",
    "            model = pickle.load(file)\n",
    "\n",
    "        alpha = model[\"alpha\"]\n",
    "        beta = model[\"beta\"]\n",
    "        multi_indices_n = model[\"multi_indices_n\"]\n",
    "        multi_indices_d = model[\"multi_indices_d\"]\n",
    "        G, H, _, _ = construct_G_H_matrices(x_train_norm, model[\"n_components\"], model[\"degree_n\"], model[\"degree_d\"])\n",
    "\n",
    "        # Predictions for training data\n",
    "        predictions = []\n",
    "        y_binary = np.where(y_binarized[:, digit] == 1, 1, 0)  # Recreate binary labels\n",
    "        for i in range(len(x_train_norm)):\n",
    "            numerator = np.dot(G[i], alpha)\n",
    "            denominator = np.dot(H[i], beta)\n",
    "            # if np.abs(denominator) < 1e-8:\n",
    "            if np.abs(denominator) == 0:\n",
    "                denominator = 1e-8  # to prevent division by zero\n",
    "            rational_value = numerator / denominator\n",
    "            predictions.append(1 if rational_value > 0.5 else 0)  # Threshold at 0\n",
    "\n",
    "        # Calculate accuracy\n",
    "        individual_accuracy = accuracy_score(y_binary, predictions) * 100\n",
    "        individual_accuracies.append((digit, individual_accuracy))\n",
    "        print(f\"Training Accuracy for Digit {digit}: {individual_accuracy:.2f}%\")\n",
    "\n",
    "    # Calculate overall training accuracy\n",
    "    overall_predictions = []\n",
    "    for i in range(len(x_train_norm)):\n",
    "        confidence_scores = []\n",
    "        for digit in range(10):\n",
    "            # Load the model for the current digit\n",
    "            model_path = f\"{models_dir}/classifier_{digit}.pkl\"\n",
    "            with open(model_path, \"rb\") as file:\n",
    "                model = pickle.load(file)\n",
    "            alpha = model[\"alpha\"]\n",
    "            beta = model[\"beta\"]\n",
    "            multi_indices_n = model[\"multi_indices_n\"]\n",
    "            multi_indices_d = model[\"multi_indices_d\"]\n",
    "            G, H, _, _ = construct_G_H_matrices(np.array([x_train_norm[i]]), model[\"n_components\"], model[\"degree_n\"], model[\"degree_d\"])\n",
    "            numerator = np.dot(G[0], alpha)\n",
    "            denominator = np.dot(H[0], beta)\n",
    "            # if np.abs(denominator) < 1e-8:\n",
    "            if np.abs(denominator) == 0:\n",
    "                denominator = 1e-8  # to prevent division by zero\n",
    "            rational_value = numerator / denominator\n",
    "            confidence_scores.append(rational_value)\n",
    "        overall_predictions.append(np.argmax(confidence_scores))\n",
    "\n",
    "    overall_training_accuracy = accuracy_score(y_train_subset, overall_predictions) * 100\n",
    "\n",
    "    print(f\"\\nOverall Training Accuracy: {overall_training_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954a7ce-bccf-4c03-8e62-9a71fc192fb9",
   "metadata": {},
   "source": [
    "🌿Visual Cross-verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba3368-91a9-498f-b336-09783d66c60b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Specify the range of indices to visualize\n",
    "start_index = 150\n",
    "end_index = 200\n",
    "\n",
    "# Number of rows and columns for the grid\n",
    "rows = int(np.ceil((end_index - start_index) / 5))  # 5 images per row\n",
    "cols = 5\n",
    "\n",
    "# Create a figure\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
    "\n",
    "# Flatten axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through the specified range\n",
    "for idx, ax in zip(range(start_index, end_index), axes):\n",
    "    # Predicted and true labels for the current index\n",
    "    predicted_digit = overall_predictions[idx]\n",
    "    true_digit = y_train_subset[idx]\n",
    "\n",
    "    # Reshape the flattened image back to its original shape (28x28)\n",
    "    image = x_train_subset[idx].reshape(28, 28)\n",
    "\n",
    "    # Determine correctness\n",
    "    if predicted_digit == true_digit:\n",
    "        status = \"correct✔️\"\n",
    "    else:\n",
    "        status = \"wrong✖️\"\n",
    "    \n",
    "    # Plot the image\n",
    "    ax.imshow(image, cmap='gray')  # Use 'gray' colormap for MNIST\n",
    "    ax.set_title(f\"Pred: {predicted_digit}, True: {true_digit}\\n{status}\")\n",
    "    ax.axis(\"off\")  # Hide axes for cleaner visuals\n",
    "\n",
    "# Turn off unused subplots\n",
    "for ax in axes[len(range(start_index, end_index)):]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SageMath)",
   "language": "python",
   "name": "sagemath-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
